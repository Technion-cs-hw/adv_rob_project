# Adversarial Robustness of Transformer-Based Image Captioning Models
This project was done as fulfillment of the requirements of the 236874 Technion course on computer vision, in the winter semester of the academic year 2024/25, under the supervision of Ori Bryt.


It benchmarks state-of-the-art image captioning models and analyzes their robustness against adversarial attacks. We explore how well models such as BLIP and ExpansionNet handle both natural and adversarial perturbations, and propose two defense mechanisms: **Gaussian blur** and a **caption confidence metric**. The project also includes a classification-based sanity check and a human evaluation study.

## ğŸ“Œ Features

- Benchmarking of popular image captioning models (BLIP, ExpansionNet)
- Evaluation on both natural corruptions (blur, noise, geometric transforms) and adversarial perturbations
- Defense methods:
  - **Gaussian blur** applied to input images
  - **Caption confidence** based on inverse perplexity of output logits
- Human study comparing captioning performance under noise
- Sanity check using CLIP encoder + linear classifier on Caltech-101
- Qualitative and quantitative results

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ adversarial_attacks/        # adversarial attack results
â”œâ”€â”€ benchmarks/                 # basic perturbation results
â”œâ”€â”€ projectCode/                # Core model logic and dataset wrappers
â”‚   â”œâ”€â”€ CartoonDataset.py       # Cartoon dataset loader (used for OOD evaluation)
â”‚   â”œâ”€â”€ CocoDataset.py          # COCO dataset wrapper
â”‚   â”œâ”€â”€ Evaluator.py            # Evaluation pipeline
â”‚   â”œâ”€â”€ ModelLoader.py          # Unified model loading (BLIP, ExpansionNet, ...)
â”‚   â”œâ”€â”€ Perturbator.py          # Natural image corruptions (blur, noise, transforms)
â”‚   â”œâ”€â”€ attack.py               # Attack entry point
â”‚   â”œâ”€â”€ imagenet100.py          # ImageNet100 class utility (if used)
â”‚   â”œâ”€â”€ imports.py              # Centralized imports and configs
â”‚   â”œâ”€â”€ utils.py                # Helper functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ Blip_Benchmark.ipynb          # BLIP model benchmarking notebook
â”œâ”€â”€ Blip_Classification.ipynb     # Sanity check: BLIP classifier on Caltech-101
â”œâ”€â”€ ExpansionNet_Attack.ipynb     # Adversarial attack on ExpansionNet
â”œâ”€â”€ ExpansionNet_benchmark.ipynb  # Benchmarking ExpansionNet
â”œâ”€â”€ ExpansionNet_confidence.ipynb # Confidence-based defense on ExpansionNet
â”œâ”€â”€ Plot_results.ipynb            # Visualization of evaluation metrics and model behavior
â”œâ”€â”€ README.md
```


## ğŸ“Š Results

- Adversarial perturbations (even small) cause major semantic degradation in captioning.
- Blur defends surprisingly well when dealing with captioning tasks.
- Caption confidence is useful for filtering unreliable predictions.
- Classifier-based sanity check confirms literature: transformer classifiers are highly vulnerable to adversarial attacks.
- Human study shows performance degrades with noise, but humans are more robust than models.

## ğŸ“‚ External Resources Required

âš ï¸ **Important:** The following resources must be downloaded separately and placed in the specified directories:

- **ExpansionNet_v2 weights:**  
  Place in: `models/ExpansionNet_v2/`

- **COCO annotations:**  
  Place in: `annotations/annotations/`

- **Caltech-101 dataset:**  
  Place in: `datasets/caltech101/`

## ğŸ§ª Reproducibility

- All experiments are run with fixed seeds.
- We provide scripts for reproducibility across:
  - Natural corruption evaluation
  - AutoAttack generation
  - Blur and confidence-based defenses
  - Classifier sanity check

## ğŸ“ Citation

If you use this project or its components in your research, please consider citing our work.

---
